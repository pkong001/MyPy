{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "my_input = \"\"\"\n",
    "1\n",
    "\n",
    "41:34\n",
    "01. Databricks: Spark Architecture & Internal Working Mechanism\n",
    "Raja's Data Engineering\n",
    "\n",
    "2\n",
    "\n",
    "12:41\n",
    "02. Databricks | PySpark: RDD, Dataframe and Dataset\n",
    "Raja's Data Engineering\n",
    "\n",
    "3\n",
    "\n",
    "16:15\n",
    "03. Databricks | PySpark: Transformation and Action\n",
    "Raja's Data Engineering\n",
    "\n",
    "4\n",
    "\n",
    "11:56\n",
    "04. On-Heap vs Off-Heap| Databricks | Spark | Interview Question | Performance Tuning\n",
    "Raja's Data Engineering\n",
    "\n",
    "5\n",
    "\n",
    "15:08\n",
    "05. Databricks | Pyspark: Cluster Deployment\n",
    "Raja's Data Engineering\n",
    "\n",
    "6\n",
    "\n",
    "17:06\n",
    "06. Databricks | Pyspark| Spark Reader: Read CSV File\n",
    "Raja's Data Engineering\n",
    "\n",
    "7\n",
    "\n",
    "14:28\n",
    "07. Databricks | Pyspark: Filter Condition\n",
    "Raja's Data Engineering\n",
    "\n",
    "8\n",
    "\n",
    "12:03\n",
    "08. Databricks | Pyspark: Add, Rename and Drop Columns\n",
    "Raja's Data Engineering\n",
    "\n",
    "9\n",
    "\n",
    "14:28\n",
    "09. Databricks | PySpark Join Types\n",
    "Raja's Data Engineering\n",
    "\n",
    "10\n",
    "\n",
    "28:43\n",
    "10. Databricks | Pyspark: Utility Commands - DBUtils\n",
    "Raja's Data Engineering\n",
    "\n",
    "11\n",
    "\n",
    "15:24\n",
    "11. Databricks | Pyspark: Explode Function\n",
    "Raja's Data Engineering\n",
    "\n",
    "12\n",
    "\n",
    "13:34\n",
    "12. Databricks | Pyspark: Case Function (When.Otherwise )\n",
    "Raja's Data Engineering\n",
    "\n",
    "13\n",
    "\n",
    "10:25\n",
    "13. Databricks | Pyspark: Union & UnionAll\n",
    "Raja's Data Engineering\n",
    "\n",
    "14\n",
    "\n",
    "11:33\n",
    "14. Databricks | Pyspark: Pivot & Unpivot\n",
    "Raja's Data Engineering\n",
    "\n",
    "15\n",
    "\n",
    "9:35\n",
    "15. Databricks| Spark | Pyspark | Read Json| Flatten Json\n",
    "Raja's Data Engineering\n",
    "\n",
    "16\n",
    "\n",
    "7:24\n",
    "16. Databricks | Spark | Pyspark | Bad Records Handling | Permissive;DropMalformed;FailFast\n",
    "Raja's Data Engineering\n",
    "\n",
    "17\n",
    "\n",
    "14:43\n",
    "17. Databricks & Pyspark: Azure Data Lake Storage Integration with Databricks\n",
    "Raja's Data Engineering\n",
    "\n",
    "18\n",
    "\n",
    "12:08\n",
    "18. Databricks & Pyspark: Ingest Data from Azure SQL Database\n",
    "Raja's Data Engineering\n",
    "\n",
    "19\n",
    "\n",
    "17:04\n",
    "19. Databricks & Pyspark: Real Time ETL Pipeline Azure SQL to ADLS\n",
    "Raja's Data Engineering\n",
    "\n",
    "20\n",
    "\n",
    "9:32\n",
    "20. Databricks & Pyspark: Azure Key Vault Integration\n",
    "Raja's Data Engineering\n",
    "\n",
    "21\n",
    "\n",
    "18:12\n",
    "21. Databricks| Spark Streaming\n",
    "Raja's Data Engineering\n",
    "\n",
    "22\n",
    "\n",
    "21:11\n",
    "22. Databricks| Spark | Performance Optimization | Repartition vs Coalesce\n",
    "Raja's Data Engineering\n",
    "\n",
    "23\n",
    "\n",
    "18:56\n",
    "23. Databricks | Spark | Cache vs Persist | Interview Question | Performance Tuning\n",
    "Raja's Data Engineering\n",
    "\n",
    "24\n",
    "\n",
    "19:42\n",
    "24. Databricks| Spark | Interview Questions| Catalyst Optimizer\n",
    "Raja's Data Engineering\n",
    "\n",
    "25\n",
    "\n",
    "13:33\n",
    "25. Databricks | Spark | Broadcast Variable| Interview Question | Performance Tuning\n",
    "Raja's Data Engineering\n",
    "\n",
    "26\n",
    "\n",
    "17:14\n",
    "26. Databricks | Spark | Adaptive Query Execution| Interview Question | Performance Tuning\n",
    "Raja's Data Engineering\n",
    "\n",
    "27\n",
    "\n",
    "10:05\n",
    "31. Databricks Pyspark: Handling Null - Part1\n",
    "Raja's Data Engineering\n",
    "\n",
    "28\n",
    "\n",
    "14:17\n",
    "32. Databricks| Pyspark| Handling Null Part 2\n",
    "Raja's Data Engineering\n",
    "\n",
    "29\n",
    "\n",
    "10:08\n",
    "33. Databricks | Spark | Pyspark | UDF\n",
    "Raja's Data Engineering\n",
    "\n",
    "30\n",
    "\n",
    "15:03\n",
    "34. Databricks - Spark: Data Skew Optimization\n",
    "Raja's Data Engineering\n",
    "\n",
    "31\n",
    "\n",
    "5:52\n",
    "35. Databricks & Spark: Interview Question - Shuffle Partition\n",
    "Raja's Data Engineering\n",
    "\n",
    "32\n",
    "\n",
    "6:00\n",
    "36. Databricks: Autoscaling | Optimized Autoscaling\n",
    "Raja's Data Engineering\n",
    "\n",
    "33\n",
    "\n",
    "8:14\n",
    "37. Databricks | Pyspark: Dataframe Checkpoint\n",
    "Raja's Data Engineering\n",
    "\n",
    "34\n",
    "\n",
    "10:30\n",
    "38. Databricks | Pyspark | Interview Question | Compression Methods: Snappy vs Gzip\n",
    "Raja's Data Engineering\n",
    "\n",
    "35\n",
    "\n",
    "10:41\n",
    "39. Databricks | Spark | Pyspark Functions| Split\n",
    "Raja's Data Engineering\n",
    "\n",
    "36\n",
    "\n",
    "17:14\n",
    "40. Databricks | Spark | Pyspark Functions| Arrays_zip\n",
    "Raja's Data Engineering\n",
    "\n",
    "37\n",
    "\n",
    "5:11\n",
    "41. Databricks | Spark | Pyspark Functions| Part 2 : Array_Intersect\n",
    "Raja's Data Engineering\n",
    "\n",
    "38\n",
    "\n",
    "4:48\n",
    "42. Databricks | Spark | Pyspark Functions| Part 3 : Array_Except\n",
    "Raja's Data Engineering\n",
    "\n",
    "39\n",
    "\n",
    "4:12\n",
    "43. Databricks | Spark | Pyspark Functions| Part 4 : Array_Sort\n",
    "Raja's Data Engineering\n",
    "\n",
    "40\n",
    "\n",
    "7:24\n",
    "44. Databricks | Spark | Python Functions| Join\n",
    "Raja's Data Engineering\n",
    "\n",
    "41\n",
    "\n",
    "13:09\n",
    "45. Databricks | Spark | Pyspark | PartitionBy\n",
    "Raja's Data Engineering\n",
    "\n",
    "42\n",
    "\n",
    "5:53\n",
    "46. Databricks | Spark | Pyspark | Number of Records per Partition in Dataframe\n",
    "Raja's Data Engineering\n",
    "\n",
    "43\n",
    "\n",
    "2:59\n",
    "47. Databricks | Spark | Pyspark | Null Count of Each Column in Dataframe\n",
    "Raja's Data Engineering\n",
    "\n",
    "44\n",
    "\n",
    "9:08\n",
    "48. Databricks - Pyspark: Find Top or Bottom N Rows per Group\n",
    "Raja's Data Engineering\n",
    "\n",
    "45\n",
    "\n",
    "6:01\n",
    "49. Databricks & Spark: Interview Question(Scenario Based) - How many spark jobs get created?\n",
    "Raja's Data Engineering\n",
    "\n",
    "46\n",
    "\n",
    "6:56\n",
    "50. Databricks | Pyspark: Greatest vs Least vs Max vs Min\n",
    "Raja's Data Engineering\n",
    "\n",
    "47\n",
    "\n",
    "10:27\n",
    "51. Databricks | Pyspark | Delta Lake: Introduction to Delta Lake\n",
    "Raja's Data Engineering\n",
    "\n",
    "48\n",
    "\n",
    "30:13\n",
    "52. Databricks| Pyspark| Delta Lake Architecture: Internal Working Mechanism\n",
    "Raja's Data Engineering\n",
    "\n",
    "49\n",
    "\n",
    "7:53\n",
    "53. Databricks| Pyspark| Delta Lake: Solution Architecture\n",
    "Raja's Data Engineering\n",
    "\n",
    "50\n",
    "\n",
    "11:56\n",
    "54. Databricks | Delta Lake| Pyspark: Create Delta Table Using Various Methods\n",
    "Raja's Data Engineering\n",
    "\n",
    "51\n",
    "\n",
    "11:29\n",
    "55. Databricks| Pyspark| Delta Lake: Delta Table Instance\n",
    "Raja's Data Engineering\n",
    "\n",
    "52\n",
    "\n",
    "10:47\n",
    "56. Databricks| Pyspark | Delta Lake: Different Approaches to Insert Data Into Delta Table\n",
    "Raja's Data Engineering\n",
    "\n",
    "53\n",
    "\n",
    "8:44\n",
    "57. Databricks| Pyspark| Delta Lake: Different Approaches to Delete Data from Delta Table\n",
    "Raja's Data Engineering\n",
    "\n",
    "54\n",
    "\n",
    "7:49\n",
    "58. Databricks | Pyspark | Delta Lake : Update Delta Table\n",
    "Raja's Data Engineering\n",
    "\n",
    "55\n",
    "\n",
    "11:30\n",
    "59. Databricks Pyspark:Slowly Changing Dimension|SCD Type1| Merge using Pyspark and Spark SQL\n",
    "Raja's Data Engineering\n",
    "\n",
    "56\n",
    "\n",
    "17:12\n",
    "60. Databricks & Pyspark: Delta Lake Audit Log Table with Operation Metrics\n",
    "Raja's Data Engineering\n",
    "\n",
    "57\n",
    "\n",
    "20:03\n",
    "61. Databricks | Pyspark | Delta Lake : Slowly Changing Dimension (SCD Type2)\n",
    "Raja's Data Engineering\n",
    "\n",
    "58\n",
    "\n",
    "8:47\n",
    "62. Databricks | Pyspark | Delta Lake: Time Travel\n",
    "Raja's Data Engineering\n",
    "\n",
    "59\n",
    "\n",
    "7:28\n",
    "63. Databricks | Pyspark| Delta Lake: Restore Command\n",
    "Raja's Data Engineering\n",
    "\n",
    "60\n",
    "\n",
    "13:16\n",
    "64. Databricks | Pyspark | Delta Lake: Optimize Command - File Compaction\n",
    "Raja's Data Engineering\n",
    "\n",
    "61\n",
    "\n",
    "15:32\n",
    "65. Databricks | Pyspark | Delta Lake: Vacuum Command\n",
    "Raja's Data Engineering\n",
    "\n",
    "62\n",
    "\n",
    "14:16\n",
    "66. Databricks | Pyspark | Delta: Z-Order Command\n",
    "Raja's Data Engineering\n",
    "\n",
    "63\n",
    "\n",
    "7:53\n",
    "67. Databricks | Pypark | Delta: Schema Evolution - MergeSchema\n",
    "Raja's Data Engineering\n",
    "\n",
    "64\n",
    "\n",
    "8:09\n",
    "68. Databricks | Pyspark | Dataframe InsertInto Delta Table\n",
    "Raja's Data Engineering\n",
    "\n",
    "65\n",
    "\n",
    "7:02\n",
    "69. Databricks | Spark | Pyspark | Data Skewness| Interview Question: SPARK_PARTITION_ID\n",
    "Raja's Data Engineering\n",
    "\n",
    "66\n",
    "\n",
    "10:47\n",
    "70. Databricks| Pyspark| Input_File_Name: Identify Input File Name of Corrupt Record\n",
    "Raja's Data Engineering\n",
    "\n",
    "67\n",
    "\n",
    "15:04\n",
    "71. Databricks | Pyspark | Window Functions: Lead and Lag\n",
    "Raja's Data Engineering\n",
    "\n",
    "68\n",
    "\n",
    "27:27\n",
    "72. Databricks | Pyspark | Interview Question: Explain Plan\n",
    "Raja's Data Engineering\n",
    "\n",
    "69\n",
    "\n",
    "6:45\n",
    "73. Databricks | Pyspark | UDF to Check if Folder Exists\n",
    "Raja's Data Engineering\n",
    "\n",
    "70\n",
    "\n",
    "16:46\n",
    "74. Databricks | Pyspark | Interview Question: Sort-Merge Join (SMJ)\n",
    "Raja's Data Engineering\n",
    "\n",
    "71\n",
    "\n",
    "22:03\n",
    "75. Databricks | Pyspark | Performance Optimization - Bucketing\n",
    "Raja's Data Engineering\n",
    "\n",
    "72\n",
    "\n",
    "8:27\n",
    "76. Databricks|Pyspark:Interview Question|Scenario Based|Max Over () Get Max value of Duplicate Data\n",
    "Raja's Data Engineering\n",
    "\n",
    "73\n",
    "\n",
    "8:34\n",
    "77. Databricks | Pyspark | Create_map(): Convert Dataframe Columns to Dictionary (Map Type)\n",
    "Raja's Data Engineering\n",
    "\n",
    "74\n",
    "\n",
    "7:47\n",
    "78. Databricks | Pyspark | Performance Optimization: Delta Cache\n",
    "Raja's Data Engineering\n",
    "\n",
    "75\n",
    "\n",
    "9:56\n",
    "79. Databricks | Pyspark | Split Array Elements into Separate Columns\n",
    "Raja's Data Engineering\n",
    "\n",
    "76\n",
    "\n",
    "12:09\n",
    "80. Databricks | Pyspark | Tips: Write Dataframe into Single File with Specific File Name\n",
    "Raja's Data Engineering\n",
    "\n",
    "77\n",
    "\n",
    "8:52\n",
    "81. Databricks | Pyspark | Workspace Object Access Control\n",
    "Raja's Data Engineering\n",
    "\n",
    "78\n",
    "\n",
    "19:12\n",
    "82. Databricks | Pyspark | Databricks Secret Scopes: Azure Key Vault Backed Secrets\n",
    "Raja's Data Engineering\n",
    "\n",
    "79\n",
    "\n",
    "17:17\n",
    "83. Databricks | Pyspark | Databricks Workflows: Job Scheduling\n",
    "Raja's Data Engineering\n",
    "\n",
    "80\n",
    "\n",
    "13:23\n",
    "84. Databricks | Pyspark | Azure Data Factory + Azure Databricks: Execute Notebook Via ADF\n",
    "Raja's Data Engineering\n",
    "\n",
    "81\n",
    "\n",
    "10:27\n",
    "85. Databricks | Pyspark | Notebook Activity in Azure Data Factory with Input Parameter\n",
    "Raja's Data Engineering\n",
    "\n",
    "82\n",
    "\n",
    "8:35\n",
    "86. Databricks | Pyspark | Notebook Activity in Azure Data Factory with Output Parameter\n",
    "Raja's Data Engineering\n",
    "\n",
    "83\n",
    "\n",
    "22:01\n",
    "87. Databricks | Pyspark | Real Time Project: ETL Pipeline Integrating ADF, ASQL, ADLS, Key Vault\n",
    "Raja's Data Engineering\n",
    "\n",
    "84\n",
    "\n",
    "13:11\n",
    "88. Databricks |Pyspark |Notebook Scheduling through Schedule Based Trigger using Azure Data Factory\n",
    "Raja's Data Engineering\n",
    "\n",
    "85\n",
    "\n",
    "15:35\n",
    "89. Databricks | Pyspark | Notebook Scheduling through Event Based Trigger using Azure Data Factory\n",
    "Raja's Data Engineering\n",
    "\n",
    "86\n",
    "\n",
    "10:13\n",
    "90. Databricks | Pyspark | Interview Question: Read Excel File with Multiple Sheets\n",
    "Raja's Data Engineering\n",
    "\n",
    "87\n",
    "\n",
    "11:41\n",
    "91. Databricks | Pyspark | Interview Question |Handlining Duplicate Data: DropDuplicates vs Distinct\n",
    "Raja's Data Engineering\n",
    "\n",
    "88\n",
    "\n",
    "11:33\n",
    "92. Databricks | Pyspark | Interview Question | Performance Optimization: Select vs WithColumn\n",
    "Raja's Data Engineering\n",
    "\n",
    "89\n",
    "\n",
    "15:40\n",
    "93. Databricks | Pyspark | Interview Question | Schema Definition: Struct Type vs Struct Field\n",
    "Raja's Data Engineering\n",
    "\n",
    "90\n",
    "\n",
    "13:22\n",
    "94. Databricks | Pyspark | Interview Question | Schema Definition: Struct Type vs Map Type\n",
    "Raja's Data Engineering\n",
    "\n",
    "91\n",
    "\n",
    "15:32\n",
    "95. Databricks | Pyspark | Schema | Different Methods of Schema Definition\n",
    "Raja's Data Engineering\n",
    "\n",
    "92\n",
    "\n",
    "12:34\n",
    "96. Databricks | Pyspark | Real Time Scenario | Schema Comparison\n",
    "Raja's Data Engineering\n",
    "\n",
    "93\n",
    "\n",
    "11:48\n",
    "97. Databricks | Pyspark | Data Security: Enforcing Column Level Encryption\n",
    "Raja's Data Engineering\n",
    "\n",
    "94\n",
    "\n",
    "9:09\n",
    "98. Databricks | Pyspark | Interview Question: Pyspark VS Pandas\n",
    "Raja's Data Engineering\n",
    "\n",
    "▶\n",
    "\n",
    "13:52\n",
    "99. Databricks | Pyspark | Real Time Use Case: Generate Test Data - Array_Repeat()\n",
    "Raja's Data Engineering\n",
    "\n",
    "96\n",
    "\n",
    "55:50\n",
    "100. Databricks | Pyspark | Spark Architecture: Internals of Partition Creation Demystified\n",
    "Raja's Data Engineering\n",
    "\n",
    "97\n",
    "\n",
    "14:06\n",
    "106.Databricks|Pyspark|Automation|Real Time Project:DataType Issue When Writing to Azure Synapse/SQL\n",
    "Raja's Data Engineering\n",
    "\n",
    "98\n",
    "\n",
    "8:37\n",
    "107. Databricks | Pyspark| Transformation: Subtract vs ExceptAll\n",
    "Raja's Data Engineering\n",
    "\n",
    "99\n",
    "\n",
    "12:27\n",
    "108. Databricks | Pyspark| Window Function: First and Last\n",
    "Raja's Data Engineering\n",
    "\n",
    "100\n",
    "\n",
    "6:31\n",
    "112. Databricks | Pyspark| Spark Reader: Skip First N Records While Reading CSV File\n",
    "Raja's Data Engineering\n",
    "\n",
    "101\n",
    "\n",
    "13:19\n",
    "113. Databricks | PySpark| Spark Reader: Skip Specific Range of Records While Reading CSV File\n",
    "Raja's Data Engineering\n",
    "\n",
    "102\n",
    "\n",
    "10:46\n",
    "116. Databricks | Pyspark| Query Dataframe Using Spark SQL\n",
    "Raja's Data Engineering\n",
    "\n",
    "103\n",
    "\n",
    "8:54\n",
    "119. Databricks | Pyspark| Spark SQL: Except Columns in Select Clause\n",
    "Raja's Data Engineering\n",
    "\n",
    "104\n",
    "\n",
    "34:56\n",
    "121. Databricks | Pyspark| AutoLoader: Incremental Data Load\n",
    "Raja's Data Engineering\n",
    "\n",
    "105\n",
    "\n",
    "24:25\n",
    "122. Databricks | Pyspark| Delta Live Table: Introduction\n",
    "Raja's Data Engineering\n",
    "\n",
    "106\n",
    "\n",
    "10:17\n",
    "123. Databricks | Pyspark| Delta Live Table: Declarative VS Procedural\n",
    "Raja's Data Engineering\n",
    "\n",
    "107\n",
    "\n",
    "15:09\n",
    "124. Databricks | Pyspark| Delta Live Table: Datasets - Tables and Views\n",
    "Raja's Data Engineering\n",
    "\n",
    "108\n",
    "\n",
    "8:16\n",
    "125. Databricks | Pyspark| Delta Live Table: Data Quality Check - Expect\n",
    "Raja's Data Engineering\n",
    "\n",
    "109\n",
    "\n",
    "8:00\n",
    "126. Databricks | Pyspark | Downloading Files from Databricks DBFS Location\n",
    "Raja's Data Engineering\n",
    "\n",
    "110\n",
    "\n",
    "15:36\n",
    "128. Databricks | Pyspark| Built-In Function: TRANSFORM\n",
    "Raja's Data Engineering\n",
    "\n",
    "111\n",
    "\n",
    "25:03\n",
    "129. Databricks | Pyspark| Delta Lake: Deletion Vectors\n",
    "Raja's Data Engineering\n",
    "\n",
    "112\n",
    "\n",
    "17:26\n",
    "130. Databricks | Pyspark| Delta Lake: Change Data Feed\n",
    "Raja's Data Engineering\n",
    "\n",
    "113\n",
    "\n",
    "11:54\n",
    "131. Databricks | Pyspark| Built-in Function: ZIP_WITH\n",
    "Raja's Data Engineering\n",
    "\n",
    "114\n",
    "\n",
    "8:46\n",
    "132: DataBricks Learning: System Variable _SQLDF\n",
    "Raja's Data Engineering\n",
    "\n",
    "\n",
    "\n",
    "55:50\n",
    "Now playing\n",
    "100. Databricks | Pyspark | Spark Architecture: Internals of Partition Creation Demystified\n",
    "Raja's Data Engineering\n",
    "12K views 1 year ago\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "my_list = [i for i in my_input.split() if ':' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "my_list_time = [i for i in my_list if i[0][0] in ('1','2','3','4','5','6','7','8','9','0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "my_list_min = [i[0:i.find(\":\")] for i in my_list_time if int(i[0:i.find(\":\")]) <= 60] \n",
    "my_list_min = [int(i) for i in my_list_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "my_list_sec = [i[i.find(\":\")+1:len(i)+1] for i in my_list_time]\n",
    "my_list_sec.remove(\"\")\n",
    "my_list_sec = list(map(int,my_list_sec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mins: 1584.55\n",
      "hours: 25.516666666666666\n"
     ]
    }
   ],
   "source": [
    "min = sum(my_list_min)\n",
    "sec = sum(my_list_sec)\n",
    "\n",
    "total_min = min + (sec/60)\n",
    "total_hour = min/60\n",
    "\n",
    "print(f'mins: {total_min}')\n",
    "print(f'hours: {total_hour}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['41:34', '12:41', '16:15', '11:56', '15:08', '17:06', '14:28', '12:03', '14:28', '28:43', '15:24', '13:34', '10:25', '11:33', '9:35', '7:24', '14:43', '12:08', '17:04', '9:32', '18:12', '21:11', '18:56', '19:42', '13:33', '17:14', '10:05', '14:17', '10:08', '15:03', '5:52', '6:00', '8:14', '10:30', '10:41', '17:14', '5:11', '4:48', '4:12', '7:24', '13:09', '5:53', '2:59', '9:08', '6:01', '6:56', '10:27', '30:13', '7:53', '11:56', '11:29', '10:47', '8:44', '7:49', '11:30', '17:12', '20:03', '8:47', '7:28', '13:16', '15:32', '14:16', '7:53', '8:09', '7:02', '10:47', '15:04', '27:27', '6:45', '16:46', '22:03', '8:27', '8:34', '7:47', '9:56', '12:09', '8:52', '19:12', '17:17', '13:23', '10:27', '8:35', '22:01', '13:11', '15:35', '10:13', '11:41', '11:33', '15:40', '13:22', '15:32', '12:34', '11:48', '9:09', '13:52', '55:50', '14:06', '8:37', '12:27', '6:31', '13:19', '10:46', '8:54', '34:56', '24:25', '10:17', '15:09', '8:16', '8:00', '15:36', '25:03', '17:26', '11:54', '8:46', '132:', '55:50']\n"
     ]
    }
   ],
   "source": [
    "print(my_list_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# for i in my_list_time:\n",
    "#     try:\n",
    "#         if int(i[i.find(\":\")+1:len(i)+1]) <= 60:\n",
    "#             output = i[i.find(\":\")+1:len(i)+1]\n",
    "#             print(f'xxxxxxx {output}')\n",
    "#         else:\n",
    "#             output = i[i.find(\":\")+1:len(i)+1]\n",
    "#             print(f'{output}')\n",
    "#     except:\n",
    "#         output = i[i.find(\":\")+1:len(i)+1]\n",
    "#         print(f'error {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# for i in my_list_time:\n",
    "#     try:\n",
    "#         if int(i[i.find(\":\")+1:len(i)+1]) <= 60:\n",
    "#             output = i[i.find(\":\")+1:len(i)+1]\n",
    "#             print(f'xxxxxxx {output}')\n",
    "#         else:\n",
    "\n",
    "#     except:\n",
    "#         output = i[i.find(\":\")+1:len(i)+1]\n",
    "#         print(f'error {output}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
